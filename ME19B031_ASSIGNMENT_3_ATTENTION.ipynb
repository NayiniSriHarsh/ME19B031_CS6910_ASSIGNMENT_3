{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52789814-99d7-4710-8bfc-ae550035b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc7dde6-59e3-4665-ac1b-856bc1a50ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a467f8a-2c35-4929-bbe7-9f16009aae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi A2I:\n",
      " {'SOS': 0, 'EOS': 1, 'ऀ': 2, 'ँ': 3, 'ं': 4, 'ः': 5, 'ऄ': 6, 'अ': 7, 'आ': 8, 'इ': 9, 'ई': 10, 'उ': 11, 'ऊ': 12, 'ऋ': 13, 'ऌ': 14, 'ऍ': 15, 'ऎ': 16, 'ए': 17, 'ऐ': 18, 'ऑ': 19, 'ऒ': 20, 'ओ': 21, 'औ': 22, 'क': 23, 'ख': 24, 'ग': 25, 'घ': 26, 'ङ': 27, 'च': 28, 'छ': 29, 'ज': 30, 'झ': 31, 'ञ': 32, 'ट': 33, 'ठ': 34, 'ड': 35, 'ढ': 36, 'ण': 37, 'त': 38, 'थ': 39, 'द': 40, 'ध': 41, 'न': 42, 'ऩ': 43, 'प': 44, 'फ': 45, 'ब': 46, 'भ': 47, 'म': 48, 'य': 49, 'र': 50, 'ऱ': 51, 'ल': 52, 'ळ': 53, 'ऴ': 54, 'व': 55, 'श': 56, 'ष': 57, 'स': 58, 'ह': 59, 'ऺ': 60, 'ऻ': 61, '़': 62, 'ऽ': 63, 'ा': 64, 'ि': 65, 'ी': 66, 'ु': 67, 'ू': 68, 'ृ': 69, 'ॄ': 70, 'ॅ': 71, 'ॆ': 72, 'े': 73, 'ै': 74, 'ॉ': 75, 'ॊ': 76, 'ो': 77, 'ौ': 78, '्': 79, 'ॎ': 80, 'ॏ': 81, 'ॐ': 82, '॑': 83, '॒': 84, '॓': 85, '॔': 86, 'ॕ': 87, 'ॖ': 88, 'ॗ': 89, 'क़': 90, 'ख़': 91, 'ग़': 92, 'ज़': 93, 'ड़': 94, 'ढ़': 95, 'फ़': 96, 'य़': 97, 'ॠ': 98, 'ॡ': 99, 'ॢ': 100, 'ॣ': 101, '।': 102, '॥': 103, '०': 104, '१': 105, '२': 106, '३': 107, '४': 108, '५': 109, '६': 110, '७': 111, '८': 112, '९': 113, '॰': 114, 'ॱ': 115, 'ॲ': 116, 'ॳ': 117, 'ॴ': 118, 'ॵ': 119, 'ॶ': 120, 'ॷ': 121, 'ॸ': 122, 'ॹ': 123, 'ॺ': 124, 'ॻ': 125, 'ॼ': 126, 'ॽ': 127, 'ॾ': 128, 'ॿ': 129}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "English A2I:\n",
      " {'SOS': 0, 'EOS': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "****************************************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hindi I2A:\n",
      " {0: 'SOS', 1: 'EOS', 2: 'ऀ', 3: 'ँ', 4: 'ं', 5: 'ः', 6: 'ऄ', 7: 'अ', 8: 'आ', 9: 'इ', 10: 'ई', 11: 'उ', 12: 'ऊ', 13: 'ऋ', 14: 'ऌ', 15: 'ऍ', 16: 'ऎ', 17: 'ए', 18: 'ऐ', 19: 'ऑ', 20: 'ऒ', 21: 'ओ', 22: 'औ', 23: 'क', 24: 'ख', 25: 'ग', 26: 'घ', 27: 'ङ', 28: 'च', 29: 'छ', 30: 'ज', 31: 'झ', 32: 'ञ', 33: 'ट', 34: 'ठ', 35: 'ड', 36: 'ढ', 37: 'ण', 38: 'त', 39: 'थ', 40: 'द', 41: 'ध', 42: 'न', 43: 'ऩ', 44: 'प', 45: 'फ', 46: 'ब', 47: 'भ', 48: 'म', 49: 'य', 50: 'र', 51: 'ऱ', 52: 'ल', 53: 'ळ', 54: 'ऴ', 55: 'व', 56: 'श', 57: 'ष', 58: 'स', 59: 'ह', 60: 'ऺ', 61: 'ऻ', 62: '़', 63: 'ऽ', 64: 'ा', 65: 'ि', 66: 'ी', 67: 'ु', 68: 'ू', 69: 'ृ', 70: 'ॄ', 71: 'ॅ', 72: 'ॆ', 73: 'े', 74: 'ै', 75: 'ॉ', 76: 'ॊ', 77: 'ो', 78: 'ौ', 79: '्', 80: 'ॎ', 81: 'ॏ', 82: 'ॐ', 83: '॑', 84: '॒', 85: '॓', 86: '॔', 87: 'ॕ', 88: 'ॖ', 89: 'ॗ', 90: 'क़', 91: 'ख़', 92: 'ग़', 93: 'ज़', 94: 'ड़', 95: 'ढ़', 96: 'फ़', 97: 'य़', 98: 'ॠ', 99: 'ॡ', 100: 'ॢ', 101: 'ॣ', 102: '।', 103: '॥', 104: '०', 105: '१', 106: '२', 107: '३', 108: '४', 109: '५', 110: '६', 111: '७', 112: '८', 113: '९', 114: '॰', 115: 'ॱ', 116: 'ॲ', 117: 'ॳ', 118: 'ॴ', 119: 'ॵ', 120: 'ॶ', 121: 'ॷ', 122: 'ॸ', 123: 'ॹ', 124: 'ॺ', 125: 'ॻ', 126: 'ॼ', 127: 'ॽ', 128: 'ॾ', 129: 'ॿ'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "English I2A:\n",
      " {0: 'SOS', 1: 'EOS', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
    "english_alphabets = [chr(alpha) for alpha in range(97, 123)]\n",
    "hindi_alphabet_size = len(hindi_alphabets)\n",
    "english_alphabet_size = len(english_alphabets)\n",
    "hindi_alpha2index = {\"SOS\": 0,\"EOS\": 1}\n",
    "english_alpha2index = {\"SOS\": 0,\"EOS\": 1}\n",
    "for index, alpha in enumerate(hindi_alphabets):\n",
    "    hindi_alpha2index[alpha] = index+2\n",
    "for index, alpha in enumerate(english_alphabets):\n",
    "    english_alpha2index[alpha] = index+2\n",
    "hindi_index2alpha = {0 : \"SOS\", 1 : \"EOS\"}\n",
    "english_index2alpha = { 0 : \"SOS\", 1 : \"EOS\"}\n",
    "for index, alpha in enumerate(hindi_alphabets):\n",
    "    hindi_index2alpha[index+2] = alpha\n",
    "for index, alpha in enumerate(english_alphabets):\n",
    "    english_index2alpha[index+2] = alpha \n",
    "print(\"Hindi A2I:\\n\", hindi_alpha2index)\n",
    "print(\"-\"*100)\n",
    "print(\"English A2I:\\n\", english_alpha2index)\n",
    "print(\"-\"*100)\n",
    "print(\"*\"*100)\n",
    "print(\"-\"*100)\n",
    "print(\"Hindi I2A:\\n\", hindi_index2alpha)\n",
    "print(\"-\"*100)\n",
    "print(\"English I2A:\\n\", english_index2alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c82b72-cc8e-49d7-992a-63b6176d2e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51200, 2) (4096, 2) (4096, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shastragaar</td>\n",
       "      <td>शस्त्रागार</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bindhya</td>\n",
       "      <td>बिन्द्या</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kirankant</td>\n",
       "      <td>किरणकांत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yagyopaveet</td>\n",
       "      <td>यज्ञोपवीत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ratania</td>\n",
       "      <td>रटानिया</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       English       Hindi\n",
       "0  shastragaar  शस्त्रागार\n",
       "1      bindhya    बिन्द्या\n",
       "2    kirankant    किरणकांत\n",
       "3  yagyopaveet   यज्ञोपवीत\n",
       "4      ratania     रटानिया"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"hin_train.csv\",header= None)\n",
    "data_train = pd.DataFrame(np.array(data_train),columns=[\"English\",\"Hindi\"])\n",
    "data_val = pd.read_csv(\"hin_valid.csv\",header= None)\n",
    "data_val = pd.DataFrame(np.array(data_val),columns=[\"English\",\"Hindi\"])\n",
    "data_test = pd.read_csv(\"hin_test.csv\",header= None)\n",
    "data_test = pd.DataFrame(np.array(data_test),columns=[\"English\",\"Hindi\"])\n",
    "print(data_train.shape,data_val.shape,data_test.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29bfc8a-daa8-42a1-a6b2-2f3d0e94570f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['shastragaar', 'bindhya', 'kirankant', ..., 'asahmaton',\n",
       "        'sulgaayin', 'anchuthengu'], dtype=object),\n",
       " array(['शस्त्रागार', 'बिन्द्या', 'किरणकांत', ..., 'असहमतों', 'सुलगायीं',\n",
       "        'अंचुतेंगु'], dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_X = np.array(data_train[\"English\"])\n",
    "data_train_y = np.array(data_train[\"Hindi\"])\n",
    "data_train_X,data_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d77aad5f-6e6f-4465-af9a-dc8b7cec7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize():\n",
    "    def __init__(self,Lang_From,Lang_To):\n",
    "        # Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
    "        self.L1 = Lang_From\n",
    "        self.L2 = Lang_To\n",
    "        self.SOS_token = 0\n",
    "        self.EOS_token = 1\n",
    "        hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
    "        english_alphabets = [chr(alpha) for alpha in range(97, 123)]\n",
    "        hindi_alphabet_size = len(hindi_alphabets)\n",
    "        english_alphabet_size = len(english_alphabets)\n",
    "        hindi_alpha2index = {\"SOS\": 0,\"EOS\": 1}\n",
    "        english_alpha2index = {\"SOS\": 0,\"EOS\": 1}\n",
    "        for index, alpha in enumerate(hindi_alphabets):\n",
    "            hindi_alpha2index[alpha] = index+2\n",
    "        for index, alpha in enumerate(english_alphabets):\n",
    "            english_alpha2index[alpha] = index+2\n",
    "        hindi_index2alpha = {0 : \"SOS\", 1 : \"EOS\"}\n",
    "        english_index2alpha = { 0 : \"SOS\", 1 : \"EOS\"}\n",
    "        for index, alpha in enumerate(hindi_alphabets):\n",
    "            hindi_index2alpha[index+2] = alpha\n",
    "        for index, alpha in enumerate(english_alphabets):\n",
    "            english_index2alpha[index+2] = alpha \n",
    "\n",
    "        self.Lang_From_Alpha_2_Index = english_alpha2index\n",
    "        self.Lang_To_Alpha_2_Index = hindi_alpha2index\n",
    "        self.Lang_From_Index_2_Alpha = english_index2alpha\n",
    "        self.Lang_To_Index_2_Alpha = hindi_index2alpha\n",
    "\n",
    "    def tensorFromWord(self,Lang, word):\n",
    "        if Lang == \"L1\":\n",
    "            indexes = [self.Lang_From_Alpha_2_Index[letter] for letter in word]\n",
    "        elif Lang == \"L2\":\n",
    "            indexes = [self.SOS_token]+[self.Lang_To_Alpha_2_Index[letter] for letter in word]\n",
    "        #print([self.EOS_token]*(30-len(indexes)))\n",
    "        indexes+=[self.EOS_token]*(30-len(indexes))\n",
    "        return torch.tensor(indexes, dtype=torch.long, device=device)#.view(-1, 1)\n",
    "\n",
    "    def tensorsFromPair(self,pair):\n",
    "        input_tensor = self.tensorFromWord(\"L1\",pair[self.L1])\n",
    "        target_tensor = self.tensorFromWord(\"L2\",pair[self.L2])\n",
    "        return (input_tensor, target_tensor)\n",
    "    def tensorsFromData(self,Data):\n",
    "        Tensors_Val = []\n",
    "        for i in tqdm(range(Data.shape[0])):\n",
    "            Tensors_Val.append(self.tensorsFromPair(Data.iloc[i]))\n",
    "        return Tensors_Val\n",
    "    def WordFromtensors(self,Lang, word):\n",
    "        if Lang == \"L1\":\n",
    "            letters = [self.Lang_From_Index_2_Alpha[letter.item()] for letter in word if ((letter.item() != EOS_token) and (letter.item() != SOS_token))]\n",
    "        elif Lang == \"L2\":\n",
    "            letters = [self.Lang_To_Index_2_Alpha[letter.item()] for letter in word if ((letter.item() != EOS_token) and (letter.item() != SOS_token))]\n",
    "        #print([self.EOS_token]*(30-len(indexes)))\n",
    "        word = ''.join(letters)\n",
    "        return word\n",
    "    def PairFromtensors(self,pair):\n",
    "        input_word = self.WordFromtensors(\"L1\",pair[0])\n",
    "        target_word = self.WordFromtensors(\"L2\",pair[1])\n",
    "        return (input_word, target_word)\n",
    "    '''def DataFromtensors(self,Data):\n",
    "        Tensors_Val = []\n",
    "        for i in tqdm(range(Data.shape[0])):\n",
    "            Tensors_Val.append(self.PairFromtensors(data_train.iloc[i]))\n",
    "        return Tensors_Val'''\n",
    "                                             \n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e08c227-e87a-4ae3-9e9d-e6ec3796ab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51200/51200 [00:09<00:00, 5207.39it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 5402.48it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 5489.39it/s]\n"
     ]
    }
   ],
   "source": [
    "T = Tokenize(\"English\",\"Hindi\")\n",
    "data_train_num = T.tensorsFromData(data_train)\n",
    "data_val_num = T.tensorsFromData(data_val)\n",
    "data_test_num = T.tensorsFromData(data_test)\n",
    "#tensorFromWord(english_alpha2index,data_train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4e93f4-2c54-43be-a042-34a1c48d52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4fda130-55bc-40f2-9bdc-0b2ad234c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=CustomDataset(data_train_num)\n",
    "valid_set=CustomDataset(data_val_num)\n",
    "test_set=CustomDataset(data_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d467c66d-77a0-46e6-b1b5-7cf228144bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "train_data_set=DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "valid_data_set=DataLoader(valid_set, batch_size=64, shuffle=False)\n",
    "test_data_set=DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e667c-ebfa-4a5e-9943-e56cbef15105",
   "metadata": {},
   "source": [
    "# With Attention Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1457a03c-4356-4fb6-83c1-15c106b3ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size,embedding_size,hidden_size,num_layers, dropouts,cell_type,bidirectional):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropouts)\n",
    "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
    "        self.cell_type = cell_type\n",
    "        self.bidirectional = bidirectional\n",
    "        if num_layers >1:\n",
    "            if self.cell_type == \"LSTM\":\n",
    "                self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"RNN\":\n",
    "                self.rnn = nn.RNN(embedding_size,hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"GRU\":\n",
    "                self.rnn = nn.GRU(embedding_size,hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "        else:\n",
    "            if self.cell_type == \"LSTM\":\n",
    "                self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"RNN\":\n",
    "                self.rnn = nn.RNN(embedding_size,hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"GRU\":\n",
    "                self.rnn = nn.GRU(embedding_size,hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "                \n",
    "    def forward(self,x):\n",
    "        # X : (seq_length,N)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding : seq_length,N,embedding_size)\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            outputs,(hidden,cell) = self.rnn(embedding)\n",
    "        else:\n",
    "            outputs,hidden = self.rnn(embedding)\n",
    "            cell = None\n",
    "        return outputs,hidden,cell\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffda7795-8534-442a-a90d-0e2e6bfd97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size,embedding_size,hidden_size,output_size,num_layers,dropouts,cell_type,bidirectional):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropouts)\n",
    "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
    "        self.cell_type = cell_type\n",
    "        self.bidirectional = bidirectional\n",
    "        if num_layers>1:            \n",
    "            if self.cell_type == \"LSTM\":\n",
    "                self.rnn = nn.LSTM((hidden_size*(1+self.bidirectional*1)+embedding_size),hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"RNN\":\n",
    "                self.rnn = nn.RNN((hidden_size*(1+self.bidirectional*1)+embedding_size),hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"GRU\":\n",
    "                self.rnn = nn.GRU((hidden_size*(1+self.bidirectional*1)+embedding_size),hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "        else:\n",
    "            if self.cell_type == \"LSTM\":\n",
    "                self.rnn = nn.LSTM((hidden_size*(1+self.bidirectional*1)+embedding_size),hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"RNN\":\n",
    "                self.rnn = nn.RNN((hidden_size*(1+self.bidirectional*1)+embedding_size),hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"GRU\":\n",
    "                self.rnn = nn.GRU((hidden_size*(1+self.bidirectional*1)+embedding_size),hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "        self.energy = nn.Linear(hidden_size*(2+self.bidirectional*1),1)\n",
    "        self.fc = nn.Linear((1+self.bidirectional*1)*hidden_size,output_size)\n",
    "        self.weights = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self,x,encoder_states,hidden,cell):\n",
    "        # x :(N) but we want (1,N)\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding : (1,N,embedding_size)\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        #print(\"Seq = \",sequence_length)\n",
    "        #print(\"Enc = \",encoder_states.shape)\n",
    "        #print(\"hidd = \",hidden.shape)\n",
    "        h_reshaped = hidden.repeat(int(sequence_length/(1+self.bidirectional)),1,1)\n",
    "        #print(int(sequence_length/(1+self.bidirectional)))\n",
    "        #print(h_reshaped.shape,encoder_states.shape)\n",
    "        energy = self.tanh(self.energy(torch.cat((h_reshaped,encoder_states),dim=2)))\n",
    "        attention_weights = self.weights(energy) \n",
    "        # attention : seq_length,N,1\n",
    "        attention = attention_weights.permute(1,2,0)\n",
    "        # attention : N,1,seq_length\n",
    "        encoder_states = encoder_states.permute(1,0,2)\n",
    "        # (N,1,hidden_size*2) --> (1,N,hidden_size*2)\n",
    "        context_vector = torch.bmm(attention,encoder_states).permute(1,0,2)\n",
    "        rnn_input = torch.cat((context_vector,embedding),dim=2)\n",
    "        \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            outputs,(hidden,cell) = self.rnn(rnn_input,(hidden,cell))\n",
    "        else:\n",
    "            outputs,hidden = self.rnn(rnn_input,hidden)\n",
    "            cell = None\n",
    "        # outputs : (1,N,hidden_size)\n",
    "        predictions = self.fc(outputs)\n",
    "        #predictions : (1,N,output_vocab_size)\n",
    "        predictions = self.softmax(predictions[0])\n",
    "        #predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions,hidden,cell,attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce2f8a4-249a-492b-a6ee-d1d74c55b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self,source,target,teacher_forceing=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        self.target_len = target.shape[0]\n",
    "        target_vocab_size = len(hindi_alpha2index)\n",
    "        \n",
    "        outputs = torch.zeros(self.target_len,batch_size,target_vocab_size).to(device)\n",
    "        encoder_states,hidden,cell = self.encoder(source)\n",
    "        \n",
    "        # Start Token\n",
    "        x = target[0]\n",
    "        for t in range(1,self.target_len):\n",
    "            output,hidden,cell,_ = self.decoder(x,encoder_states,hidden,cell)\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[t] if random.random()<teacher_forceing else best_guess\n",
    "        return outputs\n",
    "    def predict(self,source,track_attn_weights=False):\n",
    "        batch_size = source.shape[1]\n",
    "        target_vocab_size = len(hindi_alpha2index)\n",
    "        \n",
    "        outputs = torch.zeros(self.target_len,batch_size,target_vocab_size).to(device)\n",
    "        encoder_states,hidden,cell = self.encoder(source)\n",
    "        \n",
    "        # Start Token\n",
    "        x = 0*source[0]\n",
    "        \n",
    "        if track_attn_weights == False:\n",
    "            Attention_Weights = None\n",
    "            for t in range(1,self.target_len):\n",
    "                output,hidden,cell,_ = self.decoder(x,encoder_states,hidden,cell)\n",
    "                outputs[t] = output\n",
    "                best_guess = output.argmax(1)\n",
    "                x = best_guess\n",
    "        else:\n",
    "            Attention_Weights = torch.zeros([batch_size,self.target_len,self.target_len]).to(device)\n",
    "            for t in range(1,self.target_len):\n",
    "                output,hidden,cell,attention_weights = self.decoder(x,encoder_states,hidden,cell)\n",
    "                outputs[t] = output\n",
    "                best_guess = output.argmax(1)\n",
    "                x = best_guess\n",
    "                #print(Attention_Weights.shape,attention_weights.shape)\n",
    "                Attention_Weights[:,:,t] = attention_weights.permute(1,0,2).squeeze()\n",
    "            \n",
    "        return outputs,Attention_Weights\n",
    "    def find_crct_Tot(self,predicted_batch,target_batch):\n",
    "        crct,Total=0,0\n",
    "        for i in range(target_batch.shape[0]):\n",
    "            Pred = T.WordFromtensors(\"L2\",predicted_batch[i])\n",
    "            Targ = T.WordFromtensors(\"L2\",target_batch[i])\n",
    "            Total+=1\n",
    "            if Pred == Targ:\n",
    "                crct +=1\n",
    "        return crct,Total\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146ea8c0-93ad-4c2f-9e30-83072f6b7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.max_validation_Acc = 0\n",
    "\n",
    "    def early_stop(self, validation_Acc):\n",
    "        if validation_Acc > self.max_validation_Acc:\n",
    "            self.max_validation_loss = validation_Acc\n",
    "            self.counter = 0\n",
    "        elif validation_Acc < (self.max_validation_Acc + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c0b4a1-9576-4a43-9125-9ad7d3de4bfc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:08<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [1/25] : Train_loss = 0.7150063009187579, val_loss = 0.4713775743730366, val_accuracy = 0.06494140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [01:35<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [2/25] : Train_loss = 0.42464958757162097, val_loss = 0.3790081525221467, val_accuracy = 0.141845703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [01:59<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [3/25] : Train_loss = 0.33909481775015593, val_loss = 0.34348259773105383, val_accuracy = 0.19873046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [01:59<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [4/25] : Train_loss = 0.29584073353558776, val_loss = 0.31585541320964694, val_accuracy = 0.244140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [01:58<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [5/25] : Train_loss = 0.2627313363179564, val_loss = 0.3005466554313898, val_accuracy = 0.26123046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [01:58<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [6/25] : Train_loss = 0.2388267532736063, val_loss = 0.29578748997300863, val_accuracy = 0.291748046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:03<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [7/25] : Train_loss = 0.2236876374296844, val_loss = 0.28501634346321225, val_accuracy = 0.31884765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [01:58<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [8/25] : Train_loss = 0.20562048692256213, val_loss = 0.2764542903751135, val_accuracy = 0.331298828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:03<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [9/25] : Train_loss = 0.19022852116264402, val_loss = 0.2749389139935374, val_accuracy = 0.351806640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [01:59<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [10/25] : Train_loss = 0.17938253275118768, val_loss = 0.2743852334097028, val_accuracy = 0.357177734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [11/25] : Train_loss = 0.16974601186811925, val_loss = 0.2715958070475608, val_accuracy = 0.35107421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [12/25] : Train_loss = 0.15934327232651413, val_loss = 0.26745422650128603, val_accuracy = 0.372314453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [13/25] : Train_loss = 0.14954582477919756, val_loss = 0.27009892417117953, val_accuracy = 0.36279296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [14/25] : Train_loss = 0.14200662614777684, val_loss = 0.2645369537640363, val_accuracy = 0.376220703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [15/25] : Train_loss = 0.1364689914789051, val_loss = 0.2737767172511667, val_accuracy = 0.3837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [16/25] : Train_loss = 0.12784275459125638, val_loss = 0.2720655989833176, val_accuracy = 0.383056640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [17/25] : Train_loss = 0.11971094222739338, val_loss = 0.2692738033365458, val_accuracy = 0.38427734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [18/25] : Train_loss = 0.11336043867282569, val_loss = 0.2733965767547488, val_accuracy = 0.3876953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [19/25] : Train_loss = 0.10565000793430954, val_loss = 0.2748714422341436, val_accuracy = 0.39599609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [20/25] : Train_loss = 0.10025036859791726, val_loss = 0.2833806734997779, val_accuracy = 0.392333984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [21/25] : Train_loss = 0.09439966837409884, val_loss = 0.28025254467502236, val_accuracy = 0.39501953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [22/25] : Train_loss = 0.0882176443748176, val_loss = 0.28174721566028893, val_accuracy = 0.388916015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:04<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [23/25] : Train_loss = 0.08279922000132502, val_loss = 0.2969949634280056, val_accuracy = 0.39697265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:33<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [24/25] : Train_loss = 0.07832737917546183, val_loss = 0.29068760154768825, val_accuracy = 0.395263671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:03<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [25/25] : Train_loss = 0.07101300453301519, val_loss = 0.2976457641925663, val_accuracy = 0.406005859375\n"
     ]
    }
   ],
   "source": [
    "# Training Params\n",
    "num_epochs = 25\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "\n",
    "# Model Params\n",
    "input_size_encoder = len(english_alpha2index)\n",
    "input_size_decoder = len(hindi_alpha2index)\n",
    "output_size = len(hindi_alpha2index)\n",
    "encoder_embeddings_size = 512\n",
    "decoder_embeddings_size = 512\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "enc_dropout = 0.2\n",
    "dec_dropout = 0.2\n",
    "cell_type = \"LSTM\"\n",
    "bidirectional = True\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder,encoder_embeddings_size,hidden_size,num_layers,enc_dropout,cell_type,bidirectional).to(device)\n",
    "decoder_net = Decoder(input_size_decoder,decoder_embeddings_size,hidden_size,output_size,num_layers,dec_dropout,cell_type,bidirectional).to(device)\n",
    "\n",
    "model=Seq2Seq(encoder_net,decoder_net).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''encoder_net = Encoder(input_size_encoder,encoder_embeddings_size,hidden_size,num_layers,enc_dropout).to(device)\n",
    "decoder_net = Decoder(input_size_decoder,decoder_embeddings_size,hidden_size,output_size,num_layers,dec_dropout).to(device)'''\n",
    "\n",
    "model = Seq2Seq(encoder_net,decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr = learning_rate)\n",
    "pad_idx = EOS_token\n",
    "criterion = nn.CrossEntropyLoss()#ignore_index=pad_idx)\n",
    "Loss_log = []\n",
    "Max_Acc = 0\n",
    "early_stopper = EarlyStopper(patience=1, min_delta=0.001)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_data_set):\n",
    "        inp_data = batch[0].T.to(device)\n",
    "        target = batch[1].T.to(device)\n",
    "        #print(inp_data.shape)\n",
    "        #print(inp_data)\n",
    "        output = model(inp_data,target)\n",
    "        #output : (trg_len,batch_size,output_dim)\n",
    "        output = output[1:].reshape(-1,output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm = 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    Loss_log.append(epoch_loss)\n",
    "    Train_epoch_loss = epoch_loss/len(train_data_set)\n",
    "\n",
    "    Predictions_List = []\n",
    "    Total = 0\n",
    "    crct = 0\n",
    "    Val_epoch_loss = 0\n",
    "    for batch in valid_data_set:\n",
    "        inp_data = batch[0].T.to(device)\n",
    "        target = batch[1].T.to(device)\n",
    "        output,_ = model.predict(inp_data)\n",
    "        #print(output_val[2])\n",
    "        best_guess = output.argmax(2)\n",
    "        predictions = best_guess.squeeze()\n",
    "        #print(predictions.shape)\n",
    "        output = output[1:].reshape(-1,output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        loss = criterion(output,target)\n",
    "        Val_epoch_loss += loss.item()\n",
    "        for i in range(batch[1].shape[0]):\n",
    "            Pairs_P = T.PairFromtensors((batch[0][i],predictions.T[i]))\n",
    "            Pairs_T = T.PairFromtensors((batch[0][i],batch[1][i]))\n",
    "            Total+=1\n",
    "            if Pairs_P[1] == Pairs_T[1]:\n",
    "                crct +=1\n",
    "    Val_epoch_loss=Val_epoch_loss/len(valid_data_set)\n",
    "    Val_Accuracy = crct/Total\n",
    "    print(\"Epoch = [{}/{}] : Train_loss = {}, val_loss = {}, val_accuracy = {}\".format(epoch+1,num_epochs,Train_epoch_loss,Val_epoch_loss,Val_Accuracy))\n",
    "    if Val_Accuracy>Max_Acc:\n",
    "        torch.save(model.state_dict(),'ME19B031_Attn_Model.model')\n",
    "        Model_weights=copy.deepcopy(model.state_dict())\n",
    "        Max_Acc=Val_Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "664eee59-1d52-47d3-a568-6a277f37294e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(Model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f879f2ed-4c6f-4f26-b1fb-b1549f4a6992",
   "metadata": {},
   "source": [
    "# Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410c82f6-68f9-4802-8167-f039baffb99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy = 0.4052734375\n"
     ]
    }
   ],
   "source": [
    "Valid_Predictions_List = []\n",
    "Total = 0\n",
    "crct = 0\n",
    "for batch in valid_data_set:\n",
    "    inp_data = batch[0].T.to(device)\n",
    "    output_val,_ = model.predict(inp_data)\n",
    "    #print(output_val[2])\n",
    "    best_guess = output_val.argmax(2)\n",
    "    predictions = best_guess.squeeze()\n",
    "    #print(predictions.shape)\n",
    "    for i in range(batch[1].shape[0]):\n",
    "        Pairs_P = T.PairFromtensors((batch[0][i],predictions.T[i]))\n",
    "        Pairs_T = T.PairFromtensors((batch[0][i],batch[1][i]))\n",
    "        Total+=1\n",
    "        if Pairs_P[1] == Pairs_T[1]:\n",
    "            crct +=1\n",
    "        Valid_Predictions_List.append([Pairs_T[0],Pairs_T[1],Pairs_P[1]])\n",
    "print(\"Validation Accuracy =\",crct/Total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ec7f5c1-1377-4302-9eef-1a2762cfae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff= pd.DataFrame(np.array(Valid_Predictions_List),columns = [\"Source\",\"Target\",\"Predicted\"])\n",
    "dff.to_csv(\"With_Attention_Valid_Pred.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be7639b-ad45-4dee-8f85-534b37915bce",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fb4a14b-5f94-40c8-a8a0-564aa20bc39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.373779296875\n"
     ]
    }
   ],
   "source": [
    "Test_Predictions_List = []\n",
    "Total = 0\n",
    "crct = 0\n",
    "for batch in test_data_set:\n",
    "    inp_data = batch[0].T.to(device)\n",
    "    output_val,_ = model.predict(inp_data)\n",
    "    #print(output_val[2])\n",
    "    best_guess = output_val.argmax(2)\n",
    "    predictions = best_guess.squeeze()\n",
    "    #print(predictions.shape)\n",
    "    for i in range(batch[1].shape[0]):\n",
    "        Pairs_P = T.PairFromtensors((batch[0][i],predictions.T[i]))\n",
    "        Pairs_T = T.PairFromtensors((batch[0][i],batch[1][i]))\n",
    "        Total+=1\n",
    "        if Pairs_P[1] == Pairs_T[1]:\n",
    "            crct +=1\n",
    "        Test_Predictions_List.append([Pairs_T[0],Pairs_T[1],Pairs_P[1]])\n",
    "print(\"Test Accuracy =\",crct/Total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd294482-7f97-4379-9ffe-3cb47cb87142",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff= pd.DataFrame(np.array(Test_Predictions_List),columns = [\"Source\",\"Target\",\"Predicted\"])\n",
    "dff.to_csv(\"With_Attention_Test_Pred.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963f612-eff4-41b2-88e6-bd7d87831d81",
   "metadata": {},
   "source": [
    "# Heat Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "745d2677-02c5-4704-8a42-52f020fe90f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpoint = torch.load(\"ME19B031_Attn_Model.model\")\n",
    "model.load_state_dict(torch.load(\"ME19B031_Attn_Model.model\"))\n",
    "#Model_weights = checkpoint.state_dict()\n",
    "#model.load_state_dict(Model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55101cd2-a8cc-4754-a6c8-fb00c6a9a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = 1\n",
    "for batch in test_data_set:\n",
    "        inp_data = batch[0].T.to(device)\n",
    "        output_val, Weight = model.predict(inp_data,track_attn_weights=True)\n",
    "        best_guess = output_val.argmax(2)\n",
    "        predictions = best_guess.squeeze()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e767e489-ddd1-4030-89b8-c4153229938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mme19b031\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77aee352-2519-4e6d-9c39-2df61db58d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\nayin\\Documents\\JAN-MAY 2023\\FODL\\Assignment_3\\wandb\\run-20230521_162210-pvnu0ob5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/me19b031/ME19B031_CS6910_ASSIGNMENT_3_Final/runs/pvnu0ob5' target=\"_blank\">peach-dew-144</a></strong> to <a href='https://wandb.ai/me19b031/ME19B031_CS6910_ASSIGNMENT_3_Final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/me19b031/ME19B031_CS6910_ASSIGNMENT_3_Final' target=\"_blank\">https://wandb.ai/me19b031/ME19B031_CS6910_ASSIGNMENT_3_Final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/me19b031/ME19B031_CS6910_ASSIGNMENT_3_Final/runs/pvnu0ob5' target=\"_blank\">https://wandb.ai/me19b031/ME19B031_CS6910_ASSIGNMENT_3_Final/runs/pvnu0ob5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb.plots.* functions are deprecated and will be removed in a future release. Please use wandb.plot.* instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"ME19B031_CS6910_ASSIGNMENT_3_Final\", \n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      #name=f\"experiment\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      )\n",
    "\n",
    "\n",
    "#Pairs_P = T.PairFromtensors((batch[0][0],predictions.T[0]))\n",
    "Mat=Weight[-1].cpu().detach().numpy()[1:len(Pairs_P[1])+1,1:len(Pairs_P[0])+1]\n",
    "wandb.log({\"{}\".format(Pairs_P[0]): wandb.plots.HeatMap(list(Pairs_P[0]), list(Pairs_P[1]), Mat, show_text=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "681259e2-61be-4a36-b669-efcb5837c83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    Pairs_P = T.PairFromtensors((batch[0][i],predictions.T[i]))\n",
    "    #Pairs_P = T.PairFromtensors((batch[0][0],predictions.T[0]))\n",
    "    Mat=Weight[i].cpu().detach().numpy()#[1:len(Pairs_P[1])+1,1:len(Pairs_P[0])+1]\n",
    "    X_label = \n",
    "    wandb.log({\"{}\".format(Pairs_P[0]): wandb.plots.HeatMap(list(Pairs_P[0]), list(Pairs_P[1]), Mat, show_text=False)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
