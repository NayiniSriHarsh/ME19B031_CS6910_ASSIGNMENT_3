{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52789814-99d7-4710-8bfc-ae550035b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc7dde6-59e3-4665-ac1b-856bc1a50ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a467f8a-2c35-4929-bbe7-9f16009aae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi A2I:\n",
      " {'SOS': 0, 'EOS': 1, 'ऀ': 2, 'ँ': 3, 'ं': 4, 'ः': 5, 'ऄ': 6, 'अ': 7, 'आ': 8, 'इ': 9, 'ई': 10, 'उ': 11, 'ऊ': 12, 'ऋ': 13, 'ऌ': 14, 'ऍ': 15, 'ऎ': 16, 'ए': 17, 'ऐ': 18, 'ऑ': 19, 'ऒ': 20, 'ओ': 21, 'औ': 22, 'क': 23, 'ख': 24, 'ग': 25, 'घ': 26, 'ङ': 27, 'च': 28, 'छ': 29, 'ज': 30, 'झ': 31, 'ञ': 32, 'ट': 33, 'ठ': 34, 'ड': 35, 'ढ': 36, 'ण': 37, 'त': 38, 'थ': 39, 'द': 40, 'ध': 41, 'न': 42, 'ऩ': 43, 'प': 44, 'फ': 45, 'ब': 46, 'भ': 47, 'म': 48, 'य': 49, 'र': 50, 'ऱ': 51, 'ल': 52, 'ळ': 53, 'ऴ': 54, 'व': 55, 'श': 56, 'ष': 57, 'स': 58, 'ह': 59, 'ऺ': 60, 'ऻ': 61, '़': 62, 'ऽ': 63, 'ा': 64, 'ि': 65, 'ी': 66, 'ु': 67, 'ू': 68, 'ृ': 69, 'ॄ': 70, 'ॅ': 71, 'ॆ': 72, 'े': 73, 'ै': 74, 'ॉ': 75, 'ॊ': 76, 'ो': 77, 'ौ': 78, '्': 79, 'ॎ': 80, 'ॏ': 81, 'ॐ': 82, '॑': 83, '॒': 84, '॓': 85, '॔': 86, 'ॕ': 87, 'ॖ': 88, 'ॗ': 89, 'क़': 90, 'ख़': 91, 'ग़': 92, 'ज़': 93, 'ड़': 94, 'ढ़': 95, 'फ़': 96, 'य़': 97, 'ॠ': 98, 'ॡ': 99, 'ॢ': 100, 'ॣ': 101, '।': 102, '॥': 103, '०': 104, '१': 105, '२': 106, '३': 107, '४': 108, '५': 109, '६': 110, '७': 111, '८': 112, '९': 113, '॰': 114, 'ॱ': 115, 'ॲ': 116, 'ॳ': 117, 'ॴ': 118, 'ॵ': 119, 'ॶ': 120, 'ॷ': 121, 'ॸ': 122, 'ॹ': 123, 'ॺ': 124, 'ॻ': 125, 'ॼ': 126, 'ॽ': 127, 'ॾ': 128, 'ॿ': 129}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "English A2I:\n",
      " {'SOS': 0, 'EOS': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "****************************************************************************************************\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hindi I2A:\n",
      " {0: 'SOS', 1: 'EOS', 2: 'ऀ', 3: 'ँ', 4: 'ं', 5: 'ः', 6: 'ऄ', 7: 'अ', 8: 'आ', 9: 'इ', 10: 'ई', 11: 'उ', 12: 'ऊ', 13: 'ऋ', 14: 'ऌ', 15: 'ऍ', 16: 'ऎ', 17: 'ए', 18: 'ऐ', 19: 'ऑ', 20: 'ऒ', 21: 'ओ', 22: 'औ', 23: 'क', 24: 'ख', 25: 'ग', 26: 'घ', 27: 'ङ', 28: 'च', 29: 'छ', 30: 'ज', 31: 'झ', 32: 'ञ', 33: 'ट', 34: 'ठ', 35: 'ड', 36: 'ढ', 37: 'ण', 38: 'त', 39: 'थ', 40: 'द', 41: 'ध', 42: 'न', 43: 'ऩ', 44: 'प', 45: 'फ', 46: 'ब', 47: 'भ', 48: 'म', 49: 'य', 50: 'र', 51: 'ऱ', 52: 'ल', 53: 'ळ', 54: 'ऴ', 55: 'व', 56: 'श', 57: 'ष', 58: 'स', 59: 'ह', 60: 'ऺ', 61: 'ऻ', 62: '़', 63: 'ऽ', 64: 'ा', 65: 'ि', 66: 'ी', 67: 'ु', 68: 'ू', 69: 'ृ', 70: 'ॄ', 71: 'ॅ', 72: 'ॆ', 73: 'े', 74: 'ै', 75: 'ॉ', 76: 'ॊ', 77: 'ो', 78: 'ौ', 79: '्', 80: 'ॎ', 81: 'ॏ', 82: 'ॐ', 83: '॑', 84: '॒', 85: '॓', 86: '॔', 87: 'ॕ', 88: 'ॖ', 89: 'ॗ', 90: 'क़', 91: 'ख़', 92: 'ग़', 93: 'ज़', 94: 'ड़', 95: 'ढ़', 96: 'फ़', 97: 'य़', 98: 'ॠ', 99: 'ॡ', 100: 'ॢ', 101: 'ॣ', 102: '।', 103: '॥', 104: '०', 105: '१', 106: '२', 107: '३', 108: '४', 109: '५', 110: '६', 111: '७', 112: '८', 113: '९', 114: '॰', 115: 'ॱ', 116: 'ॲ', 117: 'ॳ', 118: 'ॴ', 119: 'ॵ', 120: 'ॶ', 121: 'ॷ', 122: 'ॸ', 123: 'ॹ', 124: 'ॺ', 125: 'ॻ', 126: 'ॼ', 127: 'ॽ', 128: 'ॾ', 129: 'ॿ'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "English I2A:\n",
      " {0: 'SOS', 1: 'EOS', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
    "english_alphabets = [chr(alpha) for alpha in range(97, 123)]\n",
    "hindi_alphabet_size = len(hindi_alphabets)\n",
    "english_alphabet_size = len(english_alphabets)\n",
    "hindi_alpha2index = {\"SOS\": 0,\"EOS\": 1}\n",
    "english_alpha2index = {\"SOS\": 0,\"EOS\": 1}\n",
    "for index, alpha in enumerate(hindi_alphabets):\n",
    "    hindi_alpha2index[alpha] = index+2\n",
    "for index, alpha in enumerate(english_alphabets):\n",
    "    english_alpha2index[alpha] = index+2\n",
    "hindi_index2alpha = {0 : \"SOS\", 1 : \"EOS\"}\n",
    "english_index2alpha = { 0 : \"SOS\", 1 : \"EOS\"}\n",
    "for index, alpha in enumerate(hindi_alphabets):\n",
    "    hindi_index2alpha[index+2] = alpha\n",
    "for index, alpha in enumerate(english_alphabets):\n",
    "    english_index2alpha[index+2] = alpha \n",
    "print(\"Hindi A2I:\\n\", hindi_alpha2index)\n",
    "print(\"-\"*100)\n",
    "print(\"English A2I:\\n\", english_alpha2index)\n",
    "print(\"-\"*100)\n",
    "print(\"*\"*100)\n",
    "print(\"-\"*100)\n",
    "print(\"Hindi I2A:\\n\", hindi_index2alpha)\n",
    "print(\"-\"*100)\n",
    "print(\"English I2A:\\n\", english_index2alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c82b72-cc8e-49d7-992a-63b6176d2e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51200, 2) (4096, 2) (4096, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shastragaar</td>\n",
       "      <td>शस्त्रागार</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bindhya</td>\n",
       "      <td>बिन्द्या</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kirankant</td>\n",
       "      <td>किरणकांत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yagyopaveet</td>\n",
       "      <td>यज्ञोपवीत</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ratania</td>\n",
       "      <td>रटानिया</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       English       Hindi\n",
       "0  shastragaar  शस्त्रागार\n",
       "1      bindhya    बिन्द्या\n",
       "2    kirankant    किरणकांत\n",
       "3  yagyopaveet   यज्ञोपवीत\n",
       "4      ratania     रटानिया"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"hin_train.csv\",header= None)\n",
    "data_train = pd.DataFrame(np.array(data_train),columns=[\"English\",\"Hindi\"])\n",
    "data_val = pd.read_csv(\"hin_valid.csv\",header= None)\n",
    "data_val = pd.DataFrame(np.array(data_val),columns=[\"English\",\"Hindi\"])\n",
    "data_test = pd.read_csv(\"hin_test.csv\",header= None)\n",
    "data_test = pd.DataFrame(np.array(data_test),columns=[\"English\",\"Hindi\"])\n",
    "print(data_train.shape,data_val.shape,data_test.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29bfc8a-daa8-42a1-a6b2-2f3d0e94570f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['shastragaar', 'bindhya', 'kirankant', ..., 'asahmaton',\n",
       "        'sulgaayin', 'anchuthengu'], dtype=object),\n",
       " array(['शस्त्रागार', 'बिन्द्या', 'किरणकांत', ..., 'असहमतों', 'सुलगायीं',\n",
       "        'अंचुतेंगु'], dtype=object))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_X = np.array(data_train[\"English\"])\n",
    "data_train_y = np.array(data_train[\"Hindi\"])\n",
    "data_train_X,data_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d77aad5f-6e6f-4465-af9a-dc8b7cec7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenize():\n",
    "    def __init__(self,Lang_From,Lang_To):\n",
    "        # Hindi Unicode Hex Range is 2304:2432. Source: https://en.wikipedia.org/wiki/Devanagari_(Unicode_block)\n",
    "        self.L1 = Lang_From\n",
    "        self.L2 = Lang_To\n",
    "        self.SOS_token = 0\n",
    "        self.EOS_token = 1\n",
    "        hindi_alphabets = [chr(alpha) for alpha in range(2304, 2432)]\n",
    "        english_alphabets = [chr(alpha) for alpha in range(97, 123)]\n",
    "        hindi_alphabet_size = len(hindi_alphabets)\n",
    "        english_alphabet_size = len(english_alphabets)\n",
    "        hindi_alpha2index = {\"SOS\": 0,\"EOS\": 1}\n",
    "        english_alpha2index = {\"SOS\": 0,\"EOS\": 1}\n",
    "        for index, alpha in enumerate(hindi_alphabets):\n",
    "            hindi_alpha2index[alpha] = index+2\n",
    "        for index, alpha in enumerate(english_alphabets):\n",
    "            english_alpha2index[alpha] = index+2\n",
    "        hindi_index2alpha = {0 : \"SOS\", 1 : \"EOS\"}\n",
    "        english_index2alpha = { 0 : \"SOS\", 1 : \"EOS\"}\n",
    "        for index, alpha in enumerate(hindi_alphabets):\n",
    "            hindi_index2alpha[index+2] = alpha\n",
    "        for index, alpha in enumerate(english_alphabets):\n",
    "            english_index2alpha[index+2] = alpha \n",
    "\n",
    "        self.Lang_From_Alpha_2_Index = english_alpha2index\n",
    "        self.Lang_To_Alpha_2_Index = hindi_alpha2index\n",
    "        self.Lang_From_Index_2_Alpha = english_index2alpha\n",
    "        self.Lang_To_Index_2_Alpha = hindi_index2alpha\n",
    "\n",
    "    def tensorFromWord(self,Lang, word):\n",
    "        if Lang == \"L1\":\n",
    "            indexes = [self.Lang_From_Alpha_2_Index[letter] for letter in word]\n",
    "        elif Lang == \"L2\":\n",
    "            indexes = [self.SOS_token]+[self.Lang_To_Alpha_2_Index[letter] for letter in word]\n",
    "        #print([self.EOS_token]*(30-len(indexes)))\n",
    "        indexes+=[self.EOS_token]*(30-len(indexes))\n",
    "        return torch.tensor(indexes, dtype=torch.long, device=device)#.view(-1, 1)\n",
    "\n",
    "    def tensorsFromPair(self,pair):\n",
    "        input_tensor = self.tensorFromWord(\"L1\",pair[self.L1])\n",
    "        target_tensor = self.tensorFromWord(\"L2\",pair[self.L2])\n",
    "        return (input_tensor, target_tensor)\n",
    "    def tensorsFromData(self,Data):\n",
    "        Tensors_Val = []\n",
    "        for i in tqdm(range(Data.shape[0])):\n",
    "            Tensors_Val.append(self.tensorsFromPair(Data.iloc[i]))\n",
    "        return Tensors_Val\n",
    "    def WordFromtensors(self,Lang, word):\n",
    "        if Lang == \"L1\":\n",
    "            letters = [self.Lang_From_Index_2_Alpha[letter.item()] for letter in word if ((letter.item() != EOS_token) and (letter.item() != SOS_token))]\n",
    "        elif Lang == \"L2\":\n",
    "            letters = [self.Lang_To_Index_2_Alpha[letter.item()] for letter in word if ((letter.item() != EOS_token) and (letter.item() != SOS_token))]\n",
    "        #print([self.EOS_token]*(30-len(indexes)))\n",
    "        word = ''.join(letters)\n",
    "        return word\n",
    "    def PairFromtensors(self,pair):\n",
    "        input_word = self.WordFromtensors(\"L1\",pair[0])\n",
    "        target_word = self.WordFromtensors(\"L2\",pair[1])\n",
    "        return (input_word, target_word)\n",
    "    '''def DataFromtensors(self,Data):\n",
    "        Tensors_Val = []\n",
    "        for i in tqdm(range(Data.shape[0])):\n",
    "            Tensors_Val.append(self.PairFromtensors(data_train.iloc[i]))\n",
    "        return Tensors_Val'''\n",
    "                                             \n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e08c227-e87a-4ae3-9e9d-e6ec3796ab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51200/51200 [00:11<00:00, 4596.00it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 5729.96it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 5972.87it/s]\n"
     ]
    }
   ],
   "source": [
    "T = Tokenize(\"English\",\"Hindi\")\n",
    "data_train_num = T.tensorsFromData(data_train)\n",
    "data_val_num = T.tensorsFromData(data_val)\n",
    "data_test_num = T.tensorsFromData(data_test)\n",
    "#tensorFromWord(english_alpha2index,data_train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4e93f4-2c54-43be-a042-34a1c48d52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4fda130-55bc-40f2-9bdc-0b2ad234c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=CustomDataset(data_train_num)\n",
    "valid_set=CustomDataset(data_val_num)\n",
    "test_set=CustomDataset(data_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d467c66d-77a0-46e6-b1b5-7cf228144bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set=DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "valid_data_set=DataLoader(valid_set, batch_size=64, shuffle=False)\n",
    "test_data_set=DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e667c-ebfa-4a5e-9943-e56cbef15105",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1457a03c-4356-4fb6-83c1-15c106b3ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size,embedding_size,hidden_size,num_layers, dropouts,cell_type,bidirectional):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropouts)\n",
    "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
    "        self.cell_type = cell_type\n",
    "        self.bidirectional = bidirectional\n",
    "        if num_layers >1:\n",
    "            if self.cell_type == \"LSTM\":\n",
    "                self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"RNN\":\n",
    "                self.rnn = nn.RNN(embedding_size,hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"GRU\":\n",
    "                self.rnn = nn.GRU(embedding_size,hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "        else:\n",
    "            if self.cell_type == \"LSTM\":\n",
    "                self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"RNN\":\n",
    "                self.rnn = nn.RNN(embedding_size,hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"GRU\":\n",
    "                self.rnn = nn.GRU(embedding_size,hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "                \n",
    "    def forward(self,x):\n",
    "        # X : (seq_length,N)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding : seq_length,N,embedding_size)\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            outputs,(hidden,cell) = self.rnn(embedding)\n",
    "        else:\n",
    "            outputs,hidden = self.rnn(embedding)\n",
    "            cell = None\n",
    "        return hidden,cell\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb635a-4602-4c8d-91c9-6696f792a185",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffda7795-8534-442a-a90d-0e2e6bfd97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size,embedding_size,hidden_size,output_size,num_layers,dropouts,cell_type,bidirectional):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropouts)\n",
    "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
    "        self.cell_type = cell_type\n",
    "        self.bidirectional = bidirectional\n",
    "        if num_layers>1:            \n",
    "            if self.cell_type == \"LSTM\":\n",
    "                self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"RNN\":\n",
    "                self.rnn = nn.RNN(embedding_size,hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"GRU\":\n",
    "                self.rnn = nn.GRU(embedding_size,hidden_size,num_layers,dropout=dropouts,bidirectional=self.bidirectional)\n",
    "        else:\n",
    "            if self.cell_type == \"LSTM\":\n",
    "                self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"RNN\":\n",
    "                self.rnn = nn.RNN(embedding_size,hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "            elif self.cell_type == \"GRU\":\n",
    "                self.rnn = nn.GRU(embedding_size,hidden_size,num_layers,bidirectional=self.bidirectional)\n",
    "        self.fc = nn.Linear((1+self.bidirectional*1)*hidden_size,output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self,x,hidden,cell):\n",
    "        # x :(N) but we want (1,N)\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding : (1,N,embedding_size)\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            outputs,(hidden,cell) = self.rnn(embedding,(hidden,cell))\n",
    "        else:\n",
    "            outputs,hidden = self.rnn(embedding,hidden)\n",
    "            cell = None\n",
    "        # outputs : (1,N,hidden_size)\n",
    "        predictions = self.fc(outputs)\n",
    "        #predictions : (1,N,output_vocab_size)\n",
    "        predictions = self.softmax(predictions[0])\n",
    "        #predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions,hidden,cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e169ee-e533-47e7-ba92-92f07221e5a4",
   "metadata": {},
   "source": [
    "# Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce2f8a4-249a-492b-a6ee-d1d74c55b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self,source,target,teacher_forcing=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        self.target_len = target.shape[0]\n",
    "        target_vocab_size = len(hindi_alpha2index)\n",
    "        \n",
    "        outputs = torch.zeros(self.target_len,batch_size,target_vocab_size).to(device)\n",
    "        hidden,cell = self.encoder(source)\n",
    "        \n",
    "        # Start Token\n",
    "        x = target[0]\n",
    "        for t in range(1,self.target_len):\n",
    "            output,hidden,cell = self.decoder(x,hidden,cell)\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[t] if random.random() < teacher_forcing else best_guess\n",
    "        return outputs\n",
    "    def predict(self,source):\n",
    "        batch_size = source.shape[1]\n",
    "        target_vocab_size = len(hindi_alpha2index)\n",
    "        \n",
    "        outputs = torch.zeros(self.target_len,batch_size,target_vocab_size).to(device)\n",
    "        hidden,cell = self.encoder(source)\n",
    "        \n",
    "        # Start Token\n",
    "        x = 0*source[0]\n",
    "        for t in range(1,self.target_len):\n",
    "            output,hidden,cell = self.decoder(x,hidden,cell)\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "            x = best_guess\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f8af5-d98a-4f8f-8ac2-123f0dab515d",
   "metadata": {},
   "source": [
    "# Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146ea8c0-93ad-4c2f-9e30-83072f6b7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.max_validation_Acc = 0\n",
    "\n",
    "    def early_stop(self, validation_Acc):\n",
    "        if validation_Acc > self.max_validation_Acc:\n",
    "            self.max_validation_loss = validation_Acc\n",
    "            self.counter = 0\n",
    "        elif validation_Acc < (self.max_validation_Acc + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769fe6a-6d3f-4fa2-8ac5-9b1353bb76e6",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c0b4a1-9576-4a43-9125-9ad7d3de4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Model(num_epochs = 10,learning_rate = 0.001,input_size_encoder = len(english_alpha2index),input_size_decoder = len(hindi_alpha2index),output_size = len(hindi_alpha2index),encoder_embeddings_size = 256,decoder_embeddings_size = 256,hidden_size = 512,num_enc_layers = 3,num_dec_layers = 3,enc_dropout = 0.2,dec_dropout = 0.2,cell_type = \"LSTM\",bidirectional = True):\n",
    "    encoder_net = Encoder(input_size_encoder,encoder_embeddings_size,hidden_size,num_enc_layers,enc_dropout,cell_type,bidirectional).to(device)\n",
    "    decoder_net = Decoder(input_size_decoder,decoder_embeddings_size,hidden_size,output_size,num_enc_layers,dec_dropout,cell_type,bidirectional).to(device)\n",
    "\n",
    "    model = Seq2Seq(encoder_net,decoder_net).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),lr = learning_rate)\n",
    "    pad_idx = EOS_token\n",
    "    criterion = nn.CrossEntropyLoss()#ignore_index=pad_idx)\n",
    "    Loss_log = []\n",
    "    Max_Acc=0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(train_data_set):\n",
    "            inp_data = batch[0].T.to(device)\n",
    "            target = batch[1].T.to(device)\n",
    "            #print(inp_data.shape)\n",
    "            #print(inp_data)\n",
    "            output = model(inp_data,target)\n",
    "            #output : (trg_len,batch_size,output_dim)\n",
    "            output = output[1:].reshape(-1,output.shape[2])\n",
    "            target = target[1:].reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output,target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm = 1)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        Loss_log.append(epoch_loss)\n",
    "        Train_epoch_loss = epoch_loss/len(train_data_set)\n",
    "\n",
    "        Predictions_List = []\n",
    "        Total = 0\n",
    "        crct = 0\n",
    "        Val_epoch_loss = 0\n",
    "        for batch in valid_data_set:\n",
    "            inp_data = batch[0].T.to(device)\n",
    "            target = batch[1].T.to(device)\n",
    "            output = model.predict(inp_data)\n",
    "            #print(output_val[2])\n",
    "            best_guess = output.argmax(2)\n",
    "            predictions = best_guess.squeeze()\n",
    "            #print(predictions.shape)\n",
    "            output = output[1:].reshape(-1,output.shape[2])\n",
    "            target = target[1:].reshape(-1)\n",
    "            loss = criterion(output,target)\n",
    "            Val_epoch_loss += loss.item()\n",
    "            for i in range(batch[1].shape[0]):\n",
    "                Pairs_P = T.PairFromtensors((batch[0][i],predictions.T[i]))\n",
    "                Pairs_T = T.PairFromtensors((batch[0][i],batch[1][i]))\n",
    "                Total+=1\n",
    "                if Pairs_P[1] == Pairs_T[1]:\n",
    "                    crct +=1\n",
    "        Val_epoch_loss=Val_epoch_loss/len(valid_data_set)\n",
    "        Val_Accuracy = crct/Total\n",
    "        print(\"Epoch = [{}/{}] : Train_loss = {}, val_loss = {}, val_accuracy = {}\".format(epoch,num_epochs,Train_epoch_loss,Val_epoch_loss,Val_Accuracy))\n",
    "        if Val_Accuracy>Max_Acc:\n",
    "            torch.save(model.state_dict(),'ME19B031_Attn_Model.model')\n",
    "            Model_weights=copy.deepcopy(model.state_dict())\n",
    "            Max_Acc=Val_Accuracy\n",
    "    model.load_state_dict(Model_weights)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bff6fde2-d636-4cab-97bb-8dfc6982b333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [12:33<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [0/2] : Train_loss = 0.47205864734947683, val_loss = 0.29551906511187553, val_accuracy = 0.256103515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [12:24<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = [1/2] : Train_loss = 0.2287938866764307, val_loss = 0.27811011392623186, val_accuracy = 0.3193359375\n"
     ]
    }
   ],
   "source": [
    "model = Train_Model(num_epochs=2,learning_rate = 0.001,input_size_encoder = len(english_alpha2index),input_size_decoder = len(hindi_alpha2index),output_size = len(hindi_alpha2index),encoder_embeddings_size = 256,decoder_embeddings_size = 256,hidden_size = 512,num_enc_layers = 3,num_dec_layers = 3,enc_dropout = 0.2,dec_dropout = 0.2,cell_type = \"LSTM\",bidirectional = True):)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f879f2ed-4c6f-4f26-b1fb-b1549f4a6992",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "410c82f6-68f9-4802-8167-f039baffb99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy = 0.31640625\n"
     ]
    }
   ],
   "source": [
    "Valid_Predictions_List = []\n",
    "Total = 0\n",
    "crct = 0\n",
    "for batch in valid_data_set:\n",
    "    inp_data = batch[0].T.to(device)\n",
    "    output_val = model.predict(inp_data)\n",
    "    #print(output_val[2])\n",
    "    best_guess = output_val.argmax(2)\n",
    "    predictions = best_guess.squeeze()\n",
    "    #print(predictions.shape)\n",
    "    for i in range(batch[1].shape[0]):\n",
    "        Pairs_P = T.PairFromtensors((batch[0][i],predictions.T[i]))\n",
    "        Pairs_T = T.PairFromtensors((batch[0][i],batch[1][i]))\n",
    "        Total+=1\n",
    "        if Pairs_P[1] == Pairs_T[1]:\n",
    "            crct +=1\n",
    "        Valid_Predictions_List.append([Pairs_T[0],Pairs_T[1],Pairs_P[1]])\n",
    "print(\"Validation Accuracy =\",crct/Total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e2c47-a7e9-485e-b357-e75cd0db08e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
